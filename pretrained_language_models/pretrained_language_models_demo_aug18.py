# -*- coding: utf-8 -*-
"""pretrained_language_models_demo_aug18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12XcJAbhnpgZes8vtYmeKhqmY2jVjFhKO
"""

!pip install -q transformers
!pip install -q datasets
!pip install -q umap-learn

from datasets import list_datasets

datasets = list_datasets()
print(f"There are {len(datasets)} datasets currently available on the Hub.")
print(f"The first 10 are: {datasets[:10]}")

metadata = list_datasets(with_details=True)[datasets.index("emotion")]
# Show dataset description
print("Description:", metadata.description, "\n")
# Show first 8 lines of the citation string
print("Citation:", "\n".join(metadata.citation.split("\n")[:8]))

from datasets import load_dataset

emotions = load_dataset("emotion")

# load_dataset("csv", data_files="my_texts.csv")

emotions

train_ds = emotions["train"]

train_ds

len(train_ds)

train_ds[0]

train_ds.column_names

train_ds.features

train_ds[:5]

train_ds["text"][:5]

import pandas as pd

emotions.set_format(type="pandas")
df = emotions["train"][:]
df.head()

def label_int2str(row, split):
    return emotions[split].features["label"].int2str(row)

df["label_name"] = df["label"].apply(label_int2str, split="train")

df.head()

import matplotlib.pyplot as plt

df["label_name"].value_counts(ascending=True).plot.barh()

df["Words Per Tweet"] = df["text"].str.split().apply(len)

df.head()

df.boxplot("Words Per Tweet", by='label_name', grid=False, showfliers=False,
           color='black', )

from transformers import AutoTokenizer

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenizer.vocab_size

tokenizer.special_tokens_map

tokenizer.model_max_length

encoded_str = tokenizer.encode("this is a complicatedtest")
encoded_str

for token in encoded_str:
    print(token, tokenizer.decode([token]))

import torch
from transformers import AutoModel

torch.cuda.is_available()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

device

model = AutoModel.from_pretrained(model_name).to(device)

text = "this is a test"
text_tensor = tokenizer.encode(text, return_tensors="pt").to(device)

text_tensor.shape

output = model(text_tensor)

output.last_hidden_state.shape

def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True)

emotions.reset_format()

tokenize(emotions["train"][:3])

emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)

emotions_encoded["train"].features

import numpy as np

def forward_pass(batch):
    input_ids = torch.tensor(batch["input_ids"]).to(device)
    attention_mask = torch.tensor(batch["attention_mask"]).to(device)

    with torch.no_grad():
        last_hidden_state = model(input_ids, attention_mask).last_hidden_state
        last_hidden_state = last_hidden_state.cpu().numpy()

    # Use average of unmasked hidden states for classification
    lhs_shape = last_hidden_state.shape
    boolean_mask = ~np.array(batch["attention_mask"]).astype(bool)
    boolean_mask = np.repeat(boolean_mask, lhs_shape[-1], axis=-1)
    boolean_mask = boolean_mask.reshape(lhs_shape)
    masked_mean = np.ma.array(last_hidden_state, mask=boolean_mask).mean(axis=1)
    batch["hidden_state"] = masked_mean.data
    return batch

emotions_encoded = emotions_encoded.map(forward_pass, batched=True, batch_size=16)

emotions_encoded["train"].features

X_train = np.array(emotions_encoded["train"]["hidden_state"])
X_valid = np.array(emotions_encoded["validation"]["hidden_state"])
y_train = np.array(emotions_encoded["train"]["label"])
y_valid = np.array(emotions_encoded["validation"]["label"])
X_train.shape, X_valid.shape

from umap import UMAP
from sklearn.preprocessing import MinMaxScaler

X_scaled = MinMaxScaler().fit_transform(X_train)
mapper = UMAP(n_components=2, metric="cosine").fit(X_scaled)
df_emb = pd.DataFrame(mapper.embedding_, columns=['X', 'Y'])
df_emb['label'] = y_train
df_emb.head()

fig, axes = plt.subplots(2, 3)
axes = axes.flatten()
cmaps = ["Greys", "Blues", "Oranges", "Reds", "Purples", "Greens"]
labels = ['sadness', 'joy', 'love', 'anger','fear', 'surprise']

for i, (label, cmap) in enumerate(zip(labels, cmaps)):
    df_emb_sub = df_emb.query(f"label == {i}")
    axes[i].hexbin(df_emb_sub["X"], df_emb_sub["Y"], cmap=cmap,
                   gridsize=20, linewidths=(0,))
    axes[i].set_title(label)
    axes[i].set_xticks([]), axes[i].set_yticks([])

from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression(n_jobs=-1, penalty="none")
lr_clf.fit(X_train, y_train)
lr_clf.score(X_valid, y_valid)

from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier(strategy="most_frequent")
dummy_clf.fit(X_train, y_train)
dummy_clf.score(X_valid, y_valid)

from sklearn.metrics import plot_confusion_matrix

plot_confusion_matrix(lr_clf, X_valid, y_valid, display_labels=labels, normalize='true', cmap=plt.cm.Blues);

from sklearn.metrics import classification_report
y_preds = lr_clf.predict(X_valid)
print(classification_report(y_valid, y_preds, target_names=labels))

from transformers import AutoModelForSequenceClassification

num_labels = 6
model = (
    AutoModelForSequenceClassification
    .from_pretrained(model_name, num_labels=num_labels)
    .to(device)
)

emotions_encoded.set_format(
    "torch",
    columns=["input_ids", "attention_mask", "label"]
)

emotions_encoded["train"][0]

from sklearn.metrics import accuracy_score, f1_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}

from transformers import TrainingArguments

batch_size = 64
logging_steps = len(emotions_encoded["train"]) // batch_size
training_args = TrainingArguments(
    output_dir="results",
    num_train_epochs=2,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    metric_for_best_model="f1",
    weight_decay=0.01,
    evaluation_strategy="epoch",
    disable_tqdm=False,
    logging_steps=logging_steps,
)

from transformers import Trainer

trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=emotions_encoded["train"],
                  eval_dataset=emotions_encoded["validation"])

trainer.train();

results = trainer.evaluate()

results

preds_output = trainer.predict(emotions_encoded["validation"])

preds_output.metrics

y_preds = np.argmax(preds_output.predictions, axis=1)

y_preds

class TransformerClf():
    def __init__(self, trainer):
        self._trainer = trainer
        self._estimator_type = "classifier"
    
    def predict(self, X):
        preds = self._trainer.predict(X)
        return np.argmax(preds.predictions, axis=1)

transformer_clf = TransformerClf(trainer)

plot_confusion_matrix(transformer_clf, emotions_encoded["validation"], y_valid, display_labels=labels, normalize='true', cmap=plt.cm.Blues);

print(classification_report(y_valid, y_preds, target_names=labels))

custom_tweet = "i saw a movie today and it was really good."
input_tensor = tokenizer.encode(custom_tweet, return_tensors="pt").to("cuda")
logits = model(input_tensor).logits

logits

softmax = torch.nn.Softmax(dim=1)
probs = softmax(logits)[0]
probs = probs.cpu().detach().numpy()

probs

np.sum(probs)

plt.bar(labels, 100 * probs, color='C0')
plt.title(f'Prediction for: "{custom_tweet}"')
plt.ylabel("Class probability (%)");

